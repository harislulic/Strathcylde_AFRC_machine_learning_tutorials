{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, Fourier transform was performed for all sensors and all cycles. All time signals were deduced the same length by adding the value of the last element at their original length. Amplitudes and uncertainties have been saved into .hdf5 file, which will be read in one of the next cells. Phases are neglected. 24 sensor have been taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import time\n",
    "# %pip install openpyxl\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import integrate\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from matplotlib._png import read_png\n",
    "from matplotlib.cbook import get_sample_data\n",
    "import h5py\n",
    "import PyDynamic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%pip install pymc3\n",
    "import pymc3 as pm\n",
    "%pip install arviz\n",
    "import arviz\n",
    "\n",
    "font = {'family' : 'Times New Roman', 'weight' : 'normal', 'size'   : 20}\n",
    "mpl.rcParams['figure.figsize'] = (20,10)\n",
    "mpl.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_a = h5py.File('Amplitudes.hdf5', 'r')\n",
    "hf_uap=h5py.File('Uncertainties.hdf5', 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_df=[None]*24\n",
    "UAP_df=[None]*24\n",
    "for i in range(24):\n",
    "    A_df[i]=hf_a[\"A_df\"+str(i)]\n",
    "    UAP_df[i]=hf_uap[\"UAP\"+str(i)]\n",
    "    A_df[i]=pd.DataFrame(A_df[i])\n",
    "    UAP_df[i]=pd.DataFrame(UAP_df[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, two ways of feature extraction will be presented. One approach is to calculate the means of all amplitudes at given frequencies (columns) and to choose 200 of the highest ones. This will be done with the function `sort_amplitudes_and_uncertainties`. It will sort the amplitudes´ columns in descending order, according to their means. The number of columns of amplitudes corresponds also to the number of columns of their standard squared uncertainties in UAP. As mentioned before, phases and covariance between amplitudes and phases will be neglected. This means that only columns of uncertainties of amplitudes will be sorted in the way that they follow the columns of amplitudes for which they are calculated.\n",
    "If you want to sort the whole matrix of uncertainties, please have a look at ZEMA machine learning tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of time signals with padded values is 23853 (number of points) and sampling period is 0.01 s. This will serve us to get the frequencies for our amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_sampling_pts=23853\n",
    "sample_period=0.01\n",
    "time=0.01*n_of_sampling_pts# number of sampling points\n",
    "time_steps=np.arange(0, time, 0.01)  \n",
    "freq=PyDynamic.uncertainty.propagate_DFT.GUM_DFTfreq(n_of_sampling_pts,float(time)/n_of_sampling_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_amplitudes_and_uncertainties (N,Amp,U,freq):\n",
    "\n",
    "    Amp.columns = freq                    # Column labels are frequencies. \n",
    "    n_rows, n_columns=Amp.shape\n",
    "    print(\"\\nNumber of cycles is: %s, and number of features is: %s\" % (n_rows, n_columns))\n",
    "    # Calculating the average of absolute values for each frequency (column).\n",
    "    average_values_from_columns=(Amp.mean())\n",
    "    # Sorting column indices in amplitudes for sorting phases and uncertainties\n",
    "    sorted_columns=np.argsort(average_values_from_columns)[::-1]\n",
    "    # Columns indices(len(a):3*len(a) to follow the sorting)\n",
    "    sorted_columns_unc_ap=(sorted_columns+Amp.shape[1])\n",
    "    sorted_columns_unc_pp=(sorted_columns+Amp.shape[1]*2)\n",
    "    sorted_columns_unc=np.concatenate((sorted_columns,sorted_columns_unc_ap,sorted_columns_unc_pp))#sorted indices for uncertainties\n",
    "    # Reindexing all matrices based on columns.\n",
    "    Amp=Amp.reindex(Amp.mean().sort_values(ascending=False).index, axis=1)\n",
    "    \n",
    "    c=U.reindex(columns=sorted_columns_unc)\n",
    "    \n",
    "    \n",
    "    # Taking first N percent columns from sorted amplitudes,phases and ucertainties. \n",
    "    sorted_values_amp=Amp.iloc[:,:N]\n",
    "    sorted_values_uncert_aa=c.iloc[:,:N]\n",
    "    sorted_values_uncert_ap=c.iloc[:,len(freq):Amp.shape[1]+N]\n",
    "    sorted_values_uncert_pp=c.iloc[:,2*len(freq):Amp.shape[1]*2+N]        \n",
    "                                         \n",
    "    n_rows, n_columns = np.shape(sorted_values_amp)\n",
    "    print(\"\\nNumber of cycles is: %s, and number of selected features is: %s\" % (n_rows, n_columns))\n",
    "    print(np.shape(sorted_values_amp))\n",
    "    \n",
    "    # Informations about the selected frequencies are columns in sorted data frame. \n",
    "    freq_of_sorted_values=(pd.DataFrame(sorted_values_amp.columns)).transpose()\n",
    "    print(\"\\nFirst 200 selected frequencies are:\\n\\n %s\" % freq_of_sorted_values.values[:,:N])\n",
    "    \n",
    "    # Resetting the column labels.\n",
    "    sorted_values_amp.columns=range(N)\n",
    "    \n",
    "    sorted_values_uncert_aa.columns=range(N)\n",
    "    sorted_values_uncert_ap.columns=range(N)\n",
    "    sorted_values_uncert_pp.columns=range(N)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"---------------------------------------------------------------------------------\\n\")\n",
    "    # Output \"sorted_values_matrix\" is data frame whose rows-\n",
    "    # -are cycles and columns are selected frequencies. For example,- \n",
    "    # -value at position (i,j) is amplitude for frequency j in cycle i.\n",
    "    return freq_of_sorted_values,sorted_values_amp,sorted_values_uncert_aa, sorted_values_uncert_ap,sorted_values_uncert_pp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage=input(\"Enter the percentage of spectrum to be sorted:\")\n",
    "N=(round((float(percentage)/100)*A_df[0].shape[1]))\n",
    "freq_of_sorted_values=[None]*24\n",
    "sorted_values_amp=[None]*24\n",
    "sorted_values_uncert_aa=[None]*24\n",
    "sorted_values_uncert_ap=[None]*24\n",
    "sorted_values_uncert_pp=[None]*24\n",
    "for i in range(24):\n",
    "    print(\"Sensor:\",i)\n",
    "    freq_of_sorted_values[i],sorted_values_amp[i],sorted_values_uncert_aa[i],sorted_values_uncert_ap[i],sorted_values_uncert_pp[i] =sort_amplitudes_and_uncertainties(N,A_df[i],UAP_df[i],freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An overwiev of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_of_sorted_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_values_amp[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_values_uncert_aa[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_values_uncert_ap[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_values_uncert_pp[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, an arbitrary column of measured data for all parts will be taken (because of simplicity) from the file CMMData.xlsx. For example, it will be 38 dia @200 (external diameter at 200 mm from the left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_part=pd.read_excel(Path('Data')/'AFRC Radial Forge - Zenodoo Upload v3'/'CMMData.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_part.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First three columns of `measured_part` are the nominal value and tolerances. These will be dropped, because only measured data of the parts is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target200=measured_part.iloc[3:,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson correlation between amplitudes at given frequencies (200 columns) and measured diameter at 200 mm from the left is performed in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coefs=[None]*24\n",
    "column_indices=[None]*24\n",
    "freq_indices=[None]*24\n",
    "for i in range(24):\n",
    "    corr_coefs[i]=sorted_values_amp[i].corrwith(other=target200)\n",
    "    column_indices[i]=np.argsort(np.abs(corr_coefs[i]))[::-1]\n",
    "    freq_indices[i]=freq_of_sorted_values[i][column_indices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensors with significant correlation coefficients will be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "font = {'family' : 'Times New Roman', 'weight' : 'normal', 'size'   : 20}\n",
    "mpl.rcParams['figure.figsize'] = (20,15)\n",
    "mpl.rc('font', **font)\n",
    "def sensors_correlation(i):\n",
    "    num_of_sensor=['Power [kW]', 'Force [kN]', 'A_ges_vibr', 'Schlagzahl [1/min]','A_ACTpos [mm]', 'L_ACTpos [mm]', 'R_ACTpos [mm]','SBA_ActPos [mm]', 'A_ACT_Force [kN]', 'DB_ACT_Force [kN]','L_NOMpos [mm]', 'R_NOMpos [mm]', 'INDA_NOMpos [deg]','A_NOMpos [mm]', 'Frc_Volt', 'IP_ActPos [mm]', 'IP_NomPos','RamRetract_ActSpd [rpm]', 'ForgingBox_Temp', 'TMP_Ind_U1 [°C]','TMP_Ind_F [°C]', 'W2 Durchfluss [l]', 'W1 Durchfluss [l]','L1.R_B41 [bar]']\n",
    "    plt.stem(corr_coefs[i][column_indices[i]] )\n",
    "    plt.ylabel(\"Correlation coefficients\") \n",
    "    plt.title(num_of_sensor[i])\n",
    "\n",
    "interact(sensors_correlation,i=widgets.IntSlider(min=0, max=24, step=1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of least squares is a procedure to determine the best fit line to data.\n",
    "The linear regression equation is:\n",
    "\n",
    "$$E(y) = {β}_{0} + {β}_{1}x_{1} + {β}_{2}x_{2} + · · · + {β}_{k} x_{k}$$\n",
    "\n",
    "where y denotes dependent and ${x}_{1}.....{x}_{k}$ independent variables. Finding the best fit means to estimate the coefficients ${β}_{1}.....{β}_{k}$  by minimizing the sum of squared residuals:\n",
    "$$SSE = \\sum_{i=1}^{n}[y_{i}-(\\hat{β}_{0} + \\hat{β}_{1}x_{1} + \\hat{β}_{2}x_{2} + · · · + \\hat{β}_{k} x_{k})]^2$$\n",
    "\n",
    "Sometimes it is possible to give to some observations more weight than others. It can be achieved by specifying weights and then by minimizing a weighted sum of squares:\n",
    "\n",
    "$$WSSE=\\sum_{i=1}^{n}w_{i}[y_{i}-(\\hat{β}_{0} + \\hat{β}_{1}x_{1} + \\hat{β}_{2}x_{2} + · · · + \\hat{β}_{k} x_{k})]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting β's are called weighted least squares (WLS) estimates, and the WLS residuals are:\n",
    "\n",
    "$$\\sqrt{y_{i}-\\hat{y_{i}}}$$ []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10% of the spectrum with highest average of amplitudes (columns in *sorted_values_amp*) will be analyzed through WLS method. Reciprocal values of standard squared uncertainties (variances - columns in *sorted_value_uncert_aa*) will be used as weights $w_{i}$:\n",
    "\n",
    "$$w_{i}=\\frac{1}{{u_{A}}^2}$$\n",
    "\n",
    "It is necessary to note that this will be done for the case of white noise when all values of 10% of spectrum have the same variances and weights will values of column vector $W$. For each observation, one weight value will be applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amp_corr=[None]*len(sorted_values_amp)\n",
    "UA_corr=[None]*len(sorted_values_uncert_aa)\n",
    "for i in range(len(sorted_values_amp)):\n",
    "    Amp_corr[i]=sorted_values_amp[i].iloc[:,column_indices[i]]\n",
    "    UA_corr[i]=sorted_values_uncert_aa[i].iloc[:,column_indices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[None]*len(sorted_values_amp)\n",
    "UX=[None]*len(sorted_values_uncert_aa)\n",
    "for i in range(len(sorted_values_amp)):\n",
    "    X[i]=Amp_corr[i].copy()\n",
    "    UX[i]=UA_corr[i].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With WLS, values of external diameter 200 mm from the left will be predicted. In order to do so, data will be split into train and test. WLS model will fit the train data and will be tested on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=pd.read_excel(Path('Data')/'AFRC Radial Forge - Zenodoo Upload v3'/'Data'/'CMMData.xlsx', index_col=3)\n",
    "target=target.drop(columns=['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2'])\n",
    "#target.index\n",
    "\n",
    "nominal_val=pd.DataFrame(target.iloc[0:3,:])\n",
    "target=target.iloc[3:,:]\n",
    "#Path('Data')/'AFRC Radial Forge - Zenodoo Upload v3'/'Data'/file_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=target['38 dia @200'].values\n",
    "y=pd.DataFrame(y)\n",
    "y.columns=[\"Diameter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(input(\"Enter the percentage which will be used as a traning data: \"))\n",
    "y_train, y_test=train_test_split(y, test_size=1-k/100., random_state=42)\n",
    "\n",
    "print(\"\\nNumber of cycles selected for traning is: \", y_train.shape[0],\",\")\n",
    "print(\"and number of cycles selected for testing is: \", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[0]*len(sorted_values_amp)\n",
    "X_test=[0]*len(sorted_values_amp)\n",
    "UX_train=[0]*len(sorted_values_amp)\n",
    "UX_test=[0]*len(sorted_values_amp)\n",
    "\n",
    "for i in range(len(sorted_values_amp)):\n",
    "    X_train[i]=X[i].loc[y_train.index,:]\n",
    "    UX_train[i]=UX[i].loc[y_train.index,:]\n",
    "print(\"Traning data for one sensor has dimensions: \",  X_train[0].shape,\",      ('X_train') \")\n",
    "print(\"and it's target vector has length: \", y_train.shape,\",               ('y_train') \\n\")\n",
    "\n",
    "for i in range(len(sorted_values_amp)):\n",
    "    X_test[i]=X[i].loc[y_test.index,:]\n",
    "    UX_test[i]=UX[i].loc[y_test.index,:]\n",
    "print(\"Testing data for one sensor has dimensions: \", X_test[0].shape,\",      ('X_train') \")\n",
    "print(\"and it's target vector has length: \", y_train.shape,\",               ('y_train') \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model_wls=[None]*len(X_train)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "for i in range(len(X_train)):\n",
    "    model_wls[i]=LinearRegression()\n",
    "    model_wls[i].fit(X_train[i],y_train, sample_weight=1./(UX_train[i][0].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WLS')\n",
    "print(model_wls[0].intercept_, model_wls[0].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "from sklearn.metrics import r2_score\n",
    "y_predicted = [None]*len(model_wls)\n",
    "num_of_sensor=['Power [kW]', 'Force [kN]', 'A_ges_vibr', 'Schlagzahl [1/min]','A_ACTpos [mm]', 'L_ACTpos [mm]', 'R_ACTpos [mm]','SBA_ActPos [mm]', 'A_ACT_Force [kN]', 'DB_ACT_Force [kN]','L_NOMpos [mm]', 'R_NOMpos [mm]', 'INDA_NOMpos [deg]','A_NOMpos [mm]', 'Frc_Volt', 'IP_ActPos [mm]', 'IP_NomPos','RamRetract_ActSpd [rpm]', 'ForgingBox_Temp', 'TMP_Ind_U1 [°C]','TMP_Ind_F [°C]', 'W2 Durchfluss [l]', 'W1 Durchfluss [l]','L1.R_B41 [bar]']\n",
    "for i in range((len(model_wls))):\n",
    "    y_predicted[i]=model_wls[i].predict(X_test[i])\n",
    "    #check sample weight\n",
    "    print(\"R score for the sensor\",num_of_sensor[i],\"is:\", r2_score(y_test,y_predicted[i]),\"and MSE error is:\", metrics.mean_squared_error(y_test, y_predicted[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "def sensor_predict(i):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(np.arange(0,len(y_test),1),y_test,label=\"True\")\n",
    "    plt.plot(np.arange(0,len(y_test),1),y_predicted[i],label =\"Predicted\")\n",
    "    plt.legend()\n",
    "    num_of_sensor=['Power [kW]', 'Force [kN]', 'A_ges_vibr', 'Schlagzahl [1/min]','A_ACTpos [mm]', 'L_ACTpos [mm]', 'R_ACTpos [mm]','SBA_ActPos [mm]', 'A_ACT_Force [kN]', 'DB_ACT_Force [kN]','L_NOMpos [mm]', 'R_NOMpos [mm]', 'INDA_NOMpos [deg]','A_NOMpos [mm]', 'Frc_Volt', 'IP_ActPos [mm]', 'IP_NomPos','RamRetract_ActSpd [rpm]', 'ForgingBox_Temp', 'TMP_Ind_U1 [°C]','TMP_Ind_F [°C]', 'W2 Durchfluss [l]', 'W1 Durchfluss [l]','L1.R_B41 [bar]']\n",
    " \n",
    "    plt.ylabel(\"External diameter [mm]\") \n",
    "    plt.title(num_of_sensor[i])\n",
    "\n",
    "interact(sensor_predict,i=widgets.IntSlider(min=0, max=24, step=1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(np.arange(0,len(y_test),1),y_test,label=\"True\")\n",
    "for i in range (len(y_predicted)):\n",
    "    plt.plot(np.arange(0,len(y_test),1),y_predicted[i],label =\"Predicted\" +num_of_sensor[i])\n",
    "\n",
    "num_of_sensor=['Power [kW]', 'Force [kN]', 'A_ges_vibr', 'Schlagzahl [1/min]','A_ACTpos [mm]', 'L_ACTpos [mm]', 'R_ACTpos [mm]','SBA_ActPos [mm]', 'A_ACT_Force [kN]', 'DB_ACT_Force [kN]','L_NOMpos [mm]', 'R_NOMpos [mm]', 'INDA_NOMpos [deg]','A_NOMpos [mm]', 'Frc_Volt', 'IP_ActPos [mm]', 'IP_NomPos','RamRetract_ActSpd [rpm]', 'ForgingBox_Temp', 'TMP_Ind_U1 [°C]','TMP_Ind_F [°C]', 'W2 Durchfluss [l]', 'W1 Durchfluss [l]','L1.R_B41 [bar]']\n",
    " \n",
    "plt.ylabel(\"External diameter [mm]\") \n",
    "plt.legend()\n",
    "mpl.rcParams[\"legend.loc\"] = 'best'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the outcome of the following work is not certain and it is not relied on some previous knowledge, only one sensor will be considered. This is the first sensor, that measures the power, and its that will be copied into *X* and *UX*. The observed data, that we want to predict is the external diameter of the part at 200 mm from the left. The nominal value is 38 mm. It is stored as variable *y*.\n",
    "\n",
    "Data from this sensor will be split into training and testing and stored in *X_train* and *X_test*. This will be also done for the uncertainties (*UA_train, UA_test*) and for the observed data that we want to predict (*y_train, y_test*).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Amp_corr[0].copy()\n",
    "UX=UA_corr[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target=pd.read_excel(Path('Data')/'AFRC Radial Forge - Zenodoo Upload v3'/'Data'/'CMMData.xlsx', index_col=3)\n",
    "target=target.drop(columns=['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 2'])\n",
    "#target.index\n",
    "\n",
    "nominal_val=pd.DataFrame(target.iloc[0:3,:])\n",
    "target=target.iloc[3:,:]\n",
    "#Path('Data')/'AFRC Radial Forge - Zenodoo Upload v3'/'Data'/file_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=target['38 dia @200'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.columns=[\"Diameter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UA_train,UA_test,X_train, X_test, y_train, y_test = train_test_split(UX,X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the shape of all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UA_train.shape, UA_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `draw_random_samples` obtains list *ind* of *draws* samples from multivariate normal distribution, where amplitudes are means and their uncertainties are standard deviations. These samples are matrices, with shape same as *X_train* and they are stored in the list *drawn_samples*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_samples(draws):   \n",
    "    ind=[np.zeros(X_train.shape)]*draws\n",
    "    for i in range(draws):\n",
    "        r=np.random.randn(*X_train.shape)\n",
    "        ind[i]=X_train+r*(np.sqrt(UA_train))\n",
    "        \n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawn_samples=draw_random_samples(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from *drawn_samples* will be scaled using *StandardScaler*. To be consistent with forms of functions in PyMC3, *y_train* will be inserted into *X_train_scale* as first column for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "X_train_scale=[None]*len(drawn_samples)\n",
    "for i in range(len(drawn_samples)):\n",
    "    X_train_scale[i] = scale.fit_transform(drawn_samples[i])\n",
    "    X_train_scale[i]=pd.DataFrame(X_train_scale[i])\n",
    "    X_train_scale[i].columns=list1\n",
    "    X_train_scale[i].insert(0, \"Diameter\", y_train.values, allow_duplicates = False)\n",
    "    X_train_scale[i]=X_train_scale[i].drop(X_train_scale[i].columns[10:], axis=1)\n",
    "X_test_scale=scale.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "X_train_scale=[None]*len(drawn_samples)\n",
    "for i in range(len(drawn_samples)):\n",
    "    X_train_scale[i] = scale.fit_transform(drawn_samples[i])\n",
    "    X_train_scale[i]=pd.DataFrame(X_train_scale[i])\n",
    "    X_train_scale[i].columns=list1\n",
    "    X_train_scale[i].insert(0, \"Diameter\", y_train.values, allow_duplicates = False)\n",
    "X_test_scale=scale.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UA_test_scale=scale.fit_transform(UA_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for every sample, Bayesian statistical linear model is specified. We are interested in predicting outcomes *y* (external diameter at 200 mm from the left) as normally-distributed observations with an expected value *µ* that is a linear function of  predictor variables. For example, ten predictor variables (columns) from *X_train_scale* will be taken into account for every sample: $X_{1}, X_{2}.....X_{10}$.\n",
    "The outcome *y* for one sample is:\n",
    "\n",
    "$$y ∼ N (µ, σ^2)$$\n",
    "and \n",
    "$$µ = α + β_{1}X_{1} + β_{2}X_{2}+......β_{10}X_{10}$$\n",
    "where *α* is the intercept, and $β_{i}$\n",
    "is the coefficient for covariate $X_{i}$, while *σ* represents the observation error.\n",
    "\n",
    "\n",
    "Since we are constructing a Bayesian model, the unknown variables in the model must be assigned a prior\n",
    "distribution. We choose zero-mean normal priors with variance of 10 for both regression coefficients. We choose a half-normal distribution (normal distribution bounded at zero) as the prior for σ.\n",
    "\n",
    "$$α ∼ N (0, 10)$$\n",
    "$$βi ∼ N (0, 10)$$\n",
    "$$σ ∼ |N (0, 2)|$$\n",
    "\n",
    "[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces=[None]*len(drawn_samples) #\n",
    "model_rand=[None]*len(drawn_samples) #list of model objects,one model for every sample\n",
    "for i in range(len(drawn_samples)):\n",
    "    with pm.Model() as model_rand[i]: \n",
    "        alpha = pm.Normal('alpha', mu=38, sigma=1)\n",
    "        beta = pm.Normal('beta', mu=0, sigma=100,shape=101) # shape argument to denote it as a vector-valued parameter of size 10\n",
    "        sigma = pm.HalfNormal('sigma',1)\n",
    "        mu = alpha + pm.math.dot(beta, (X_train_scale[i]).T) #Having defined the priors, the next statement creates the expected value mu of the outcomes, specifying \n",
    "        #the linear relationship:\n",
    "        #mu = alpha + beta[0]*X1 + beta[1]*X2+....beta[9]*X2\n",
    "        \n",
    "        # the sampling distribution of the outcomes in the dataset.\n",
    "        #special case of a stochastic variable that we call an observed stochastic, and represents the\n",
    "        #data likelihood of the model.\n",
    "        Diameter = pm.Normal('Diameter', mu=mu, sigma=sigma, observed=X_train_scale[i]['Diameter'])\n",
    "         # Normal constructor to create a random variable to use as a normal prior. \n",
    "        # \"Diameter\"- The first argument is always the name of the random variable, \n",
    "        # which should almost always match the name of the Python variable being assigned to\n",
    "        # mu, the mean, and sd, the standard deviation, which we assign hyperparameter values for the model. \n",
    "        traces[i] = pm.sample(2000,target_accept=0.9) #trace object containing 10 samples of posteriors for alpha,beta,sigma for every sample of X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples in *traces* are collected by No-UTurn Sampler (NUTS). NUTS is especially useful on models that have many continuous parameters, a situatiuon where other MCMC algorithms work very slowly. It takes advantage of information about where regions of\n",
    "higher probability are, based on the gradient of the log posterior-density. This helps it achieve dramatically faster convergence on large problems than traditional sampling methods achieve. PyMC3 relies on Theano to analytically compute model gradients via automatic differentation of the posterior density. NUTS also has several self-tuning strategies for adaptively setting the tunable parameters of Hamiltonian Monte Carlo. [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A posterior plot for one trace can be created using `traceplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import traceplot\n",
    "pm.traceplot(traces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left column consists of a smoothed histogram of the marginal posteriors of each stochastic random variable. The right column contains the samples of the Markov chain plotted in sequential order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary function provides a text-based output of common posterior statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import summary\n",
    "pm.summary(traces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_df=[None]*len(traces)\n",
    "X_train_scale=[None]*len(drawn_samples)\n",
    "for i in range(len(traces)):\n",
    "    traces_df[i]=pd.DataFrame(traces[i])\n",
    "    traces_df[i].to_csv(\"Traces\"+\"i\"+\".csv\")\n",
    "    X_train_scale[i].to_csv(\"X_train\"+\"i\"+\".csv\")\n",
    "X_test_scale.to_csv(\"X_test.csv\")\n",
    "y_test.to_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UA_train.to_csv(\"UA_train.csv\")\n",
    "UA_test.to_csv(\"UA_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forestplot` function supports plotting more than one trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(traces, figsize=(10, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option of plotting several traces in a same plot is to use `densityplot`. This plot is similar to a `forestplot`, but we get truncated KDE plots (by default 95% credible intervals) grouped by variable names together with a point estimate (by default the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.densityplot(traces, var_names=['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When confronted with more than one model we have several options. One of them is to perform model selection, using for example a given Information Criterion. Model selection is appealing for its simplicity, but we are discarding information about the uncertainty in our models. \n",
    "\n",
    "One alternative is to perform model selection but discuss all the different models together with the computed values of a given Information Criterion. \n",
    "\n",
    "Another approach is to perform model averaging. The idea is to generate a meta-model (and meta-predictions) using a weighted average of the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Bayesian model averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian models can be weighted by their marginal likelihood, this is known as Bayesian Model Averaging. While this is theoretically appealing, is problematic in practice: on the one hand the marginal likelihood is highly sensible to the specification of the prior, in a way that parameter estimation is not, and on the other computing the marginal likelihood is usually a challenging task. \n",
    "\n",
    "An alternative route is to use the values of WAIC (Widely Applicable Information Criterion) or LOO (pareto-smoothed importance sampling Leave-One-Out cross-validation), which we will call generically IC, to estimate weights. We can do this by using the following formula:\n",
    "\n",
    "\n",
    "$$\\frac{e^{0.5 dIC_{i}}}{\\sum_{J}^{M}{e^{-0.5*dIC_{j}}}}$$\n",
    "\n",
    "\n",
    "Where $dIC_{i}$ is the difference between the i-esim information criterion value and the lowest one. Remember that the lowest the value of the IC, the better. \n",
    "\n",
    "This approach is called pseudo Bayesian model averaging, or Akaike-like weighting and is an heuristic way to compute the relative probability of each model (given a fixed set of models) from the information criteria values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WAIC will be used to compare the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = dict(zip(model_rand, traces))\n",
    "comp = pm.compare(model_dict, method='BB-pseudo-BMA')\n",
    "model_comp=comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1=az.compare(model_dict, method='Stacking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many columns so let check one by one the meaning of them:\n",
    "\n",
    "- *WAIC* - contains the values of WAIC. The DataFrame is always sorted from lowest to highest WAIC. The index reflects the order in which the models are passed to this function.\n",
    "\n",
    "- *pWAIC* - the estimated effective number of parameters. In general, models with more parameters will be more flexible to fit data and at the same time could also lead to overfitting. Thus we can interpret pWAIC as a penalization term, intuitively we can also interpret it as measure of how flexible each model is in fitting the data.\n",
    "\n",
    "- *dWAIC* - the relative difference between the value of WAIC for the top-ranked model and the value of WAIC for each model. For this reason we will always get a value of 0 for the first model.\n",
    "\n",
    "- *weight* - can be loosely interpreted as the probability of each model (among the compared models) given the data.\n",
    "\n",
    "- *SE* - records the standard error for the WAIC computations. The standard error can be useful to assess the uncertainty of the WAIC estimates. Nevertheless, caution need to be taken because the estimation of the standard error assumes normality and hence could be problematic when the sample size is low.\n",
    "\n",
    "- *dSE* - the standard error of the differences between two values of WAIC. Notice that *SE* and *dSE* are not necessarily the same, the reason is that the uncertainty about WAIC is correlated between models. This quantity is always 0 for the top-ranked model.\n",
    "\n",
    "- *var_warn* - we have the last column named “warning”. A value of 1 indicates that the computation of WAIC may not be reliable, this warning is based on an empirical determined cutoff value and need to be interpreted with caution.[2]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use copmuted weights to generate predictions based on the weighted set of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import compare as comp\n",
    "ppc_w = pm.sample_posterior_predictive_w(traces, 1000, model_rand,weights=model_comp[\"weight\"].sort_index(ascending=True),progressbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction can be made also for the lowest-WAIC model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc_x = pm.sample_posterior_predictive(traces[2], 1000, model_rand[2],progressbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_w = ppc_w['Diameter'].mean()\n",
    "hpd_w = pm.hpd(ppc_w['Diameter']).mean(0)\n",
    "\n",
    "mean = ppc_x['Diameter'].mean()\n",
    "hpd = pm.hpd(ppc_x['Diameter']).mean(0)\n",
    "\n",
    "plt.errorbar(mean, 1,xerr=[[mean - hpd[0]]] ,fmt='o', label='model 2')\n",
    "plt.errorbar(mean_w, 0, xerr=[[mean_w - hpd_w[0]]], fmt='o', label='weighted models')\n",
    "\n",
    "plt.yticks([])\n",
    "plt.ylim(-1, 2)\n",
    "plt.xlabel('Diameter')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(traces[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_columns=np.sum((np.mean(traces[2][\"beta\"])*X_test_scale),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(0,25,1),y_test,label=\"True\")\n",
    "plt.errorbar(np.arange(0,25,1),y_test1,label =\"Predicted\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces=[None]*len(drawn_samples) #\n",
    "model_rand=[None]*len(drawn_samples) #list of model objects,one model for every sample\n",
    "for i in range(len(drawn_samples)):\n",
    "    with pm.Model() as model_rand[i]: \n",
    "        alpha = pm.Normal('alpha', mu=38, sigma=1)\n",
    "        beta = pm.Normal('beta', mu=0, sigma=100,shape=101) # shape argument to denote it as a vector-valued parameter of size 10\n",
    "        sigma = pm.HalfNormal('sigma',1)\n",
    "        mu = alpha + pm.math.dot(beta, (X_train_scale[i]).T) #Having defined the priors, the next statement creates the expected value mu of the outcomes, specifying \n",
    "        #the linear relationship:\n",
    "        #mu = alpha + beta[0]*X1 + beta[1]*X2+....beta[9]*X2\n",
    "        \n",
    "        # the sampling distribution of the outcomes in the dataset.\n",
    "        #special case of a stochastic variable that we call an observed stochastic, and represents the\n",
    "        #data likelihood of the model.\n",
    "        Diameter = pm.Normal('Diameter', mu=mu, sigma=sigma, observed=X_train_scale[i]['Diameter'])\n",
    "         # Normal constructor to create a random variable to use as a normal prior. \n",
    "        # \"Diameter\"- The first argument is always the name of the random variable, \n",
    "        # which should almost always match the name of the Python variable being assigned to\n",
    "        # mu, the mean, and sd, the standard deviation, which we assign hyperparameter values for the model. \n",
    "        traces[i] = pm.sample(2000,target_accept=0.9) #trace object containing 10 samples of posteriors for alpha,beta,sigma for every sample of X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install multinorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.tile(X_train_scale.iloc[i,1:].values, (X_train_scale.iloc[i,1:].size, 1)) + \\\n",
    "               np.random.randn(X_train_scale.iloc[i,1:].size, len(X_train_scale.iloc[i,1:])) * \\\n",
    "               np.tile(UA_train_scale.iloc[i,:].values, (UA_train_scale.iloc[i,:].size, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "X = multivariate_normal( np.arange(1,10), np.ones((10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_f = scale.fit_transform(XF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = pm.Normal('alpha', mu=38, S=1)\n",
    "alpha.dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_scale.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.zeros((X_train.shape[0]))\n",
    "\n",
    "            \n",
    "distribution=Normal_ZeroCorr(loc=X_train_scale.iloc[i,1:].values,scale=UA_train_scale.iloc[i,:].values)\n",
    "distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "F_diag=np.diagflat(UA_test_scale[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.fill_diagonal(F_diag, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_diag1=pd.DataFrame(F_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as la\n",
    "\n",
    "def nearestPD(A):\n",
    "    \"\"\"Find the nearest positive-definite matrix to input\n",
    "\n",
    "    A Python/Numpy port of John D'Errico's `nearestSPD` MATLAB code [1], which\n",
    "    credits [2].\n",
    "\n",
    "    [1] https://www.mathworks.com/matlabcentral/fileexchange/42885-nearestspd\n",
    "\n",
    "    [2] N.J. Higham, \"Computing a nearest symmetric positive semidefinite\n",
    "    matrix\" (1988): https://doi.org/10.1016/0024-3795(88)90223-6\n",
    "    \"\"\"\n",
    "\n",
    "    B = (A + A.T) / 2\n",
    "    _, s, V = la.svd(B)\n",
    "\n",
    "    H = np.dot(V.T, np.dot(np.diag(s), V))\n",
    "\n",
    "    A2 = (B + H) / 2\n",
    "\n",
    "    A3 = (A2 + A2.T) / 2\n",
    "\n",
    "    if isPD(A3):\n",
    "        return A3\n",
    "\n",
    "    spacing = np.spacing(la.norm(A))\n",
    "    # The above is different from [1]. It appears that MATLAB's `chol` Cholesky\n",
    "    # decomposition will accept matrixes with exactly 0-eigenvalue, whereas\n",
    "    # Numpy's will not. So where [1] uses `eps(mineig)` (where `eps` is Matlab\n",
    "    # for `np.spacing`), we use the above definition. CAVEAT: our `spacing`\n",
    "    # will be much larger than [1]'s `eps(mineig)`, since `mineig` is usually on\n",
    "    # the order of 1e-16, and `eps(1e-16)` is on the order of 1e-34, whereas\n",
    "    # `spacing` will, for Gaussian random matrixes of small dimension, be on\n",
    "    # othe order of 1e-16. In practice, both ways converge, as the unit test\n",
    "    # below suggests.\n",
    "    I = np.eye(A.shape[0])\n",
    "    k = 1\n",
    "    while not isPD(A3):\n",
    "        mineig = np.min(np.real(la.eigvals(A3)))\n",
    "        A3 += I * (-mineig * k**2 + spacing)\n",
    "        k += 1\n",
    "\n",
    "    return A3\n",
    "\n",
    "def isPD(B):\n",
    "    \"\"\"Returns true when input is positive-definite, via Cholesky\"\"\"\n",
    "    try:\n",
    "        _ = la.cholesky(B)\n",
    "        return True\n",
    "    except la.LinAlgError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxx= nearestPD(F_diawith pm.Model() as model:\n",
    "    # Note that we access the distribution for the standard\n",
    "    # deviations, and do not create a new random variable.\n",
    "    sd_dist = pm.HalfCauchy.dist(beta=2.5)\n",
    "    packed_chol = pm.LKJCholeskyCov('chol_cov', eta=2, n=10, sd_dist=sd_dist)\n",
    "    chol = pm.expand_packed_triangular(10, packed_chol, lower=True)\n",
    "\n",
    "    # Define a new MvNormal with the given covariance\n",
    "    vals = pm.MvNormal('vals', mu=np.zeros(10), chol=chol, shape=10)\n",
    "\n",
    "    # Or transform an uncorrelated normal:\n",
    "    vals_raw = pm.Normal('vals_raw', mu=np.zeros(10), sd=1)\n",
    "    vals = tt.dot(chol, vals_raw)\n",
    "\n",
    "    # Or compute the covariance matrix\n",
    "    cov = tt.dot(chol, chol.T)\n",
    "\n",
    "    # Extract the standard deviations\n",
    "    stds = tt.sqrt(tt.diag(cov))g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = la.cholesky(xxxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas\n",
    "\n",
    "from patsy import dmatrices\n",
    "a=sm.stats.cov_nearest(F_diag1,method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=isPD(F_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "%pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from statsmodels.iolib.table import (SimpleTable, default_txt_fmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "UA_train_scale= scale.fit_transform(UA_train)\n",
    "UA_test_scale= scale.fit_transform(UA_test)\n",
    "X_train_scale = scale.fit_transform(drawn_samples[i])\n",
    "X_train_scale=pd.DataFrame(X_train_scale)\n",
    "UA_train_scale=pd.DataFrame(UA_train_scale)\n",
    "X_train_scale.columns=list1\n",
    "X_train_scale.insert(0, \"Diameter\", y_train.values, allow_duplicates = False)\n",
    "X_test_scale=scale.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_a.close()\n",
    "hf_uap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(0,25,1),y_test,label=\"True\")\n",
    "plt.plot(np.arange(0,25,1),y_predicted,label =\"Predicted\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] John Salvatier, Thomas V. Wiecki, Christopher Fonnesbeck - \"Probabilistic Programming in Python using PyMC\", July 30, 2015\n",
    "\n",
    "[2] https://docs.pymc.io/notebooks/model_comparison.html\n",
    "\n",
    "https://docs.pymc.io/notebooks/model_averaging.html\n",
    "https://www.stat.ncsu.edu/people/bloomfield/courses/st430/slides/MandS-ch09-sec04-04.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
